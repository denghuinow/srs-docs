#!/usr/bin/env python3
"""
éœ€æ±‚æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå·¥å…·
æ”¯æŒ4ç§è¯¦ç»†ç¨‹åº¦ï¼šè¶…çŸ­æ‘˜è¦ã€ç®€çŸ­æ‘˜è¦ã€å¹³è¡¡æ‘˜è¦ã€è¯¦å°½æ‘˜è¦
"""

import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from typing import Dict, List, Optional
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ‰“å°é”ï¼Œç”¨äºçº¿ç¨‹å®‰å…¨çš„è¾“å‡º
print_lock = threading.Lock()

# æ‘˜è¦çº§åˆ«é…ç½®
SUMMARY_LEVELS = {
    "ultra_short": {
        "name": "Ultra Short Summary",
        "folder": "summary_ultra_short",
        "info_range": "5%-10%",
        "prompt_template": """You are a requirements analysis assistant. Please generate an "Ultra Short Summary" with approximately 5â€“10% of the content.

[Selection Rules]
- Only retain the highest priority facts: business goals, MVP features, key constraints, major risks.
- Each piece of information should appear only once; remove examples and implementation details.

[Output format (Markdown)]
# Ultra Short Summary
- One-sentence positioning (1 sentence)
- MVP points (â‰¤4 items; each item 1 sentence)
- Key constraints (â‰¤3 items; each item 1 sentence)
- Major risks/undecided issues (â‰¤2 items; each item 1 sentence; unknown write "Not mentioned")"""
    },
    "short": {
        "name": "Short Summary",
        "folder": "summary_short",
        "info_range": "10%-20%",
        "prompt_template": """You are a requirements analysis assistant. Please generate a "Short Summary" with approximately 10â€“20% of the content.

[Selection Rules]
- Add boundary and success criteria outlines; avoid expanding on processes and data structures.
- Each point should be limited to 1â€“2 sentences.

[Output format (Markdown)]
# Short Summary
- Background and objectives (1â€“2 sentences)
- In scope (â‰¤5 items)
- Out of scope (â‰¤5 items)
- Roles and core use cases (â‰¤3 roles; each role 1 sentence: "As a <role>, I want <action> so that <value>")
- Success metrics (â‰¤3 items)
- Major constraints (â‰¤5 items)
- Undecided issues (â‰¤5 items; unknown write "Not mentioned")"""
    },
    "balanced": {
        "name": "Balanced Summary",
        "folder": "summary_balanced",
        "info_range": "20%-30%",
        "prompt_template": """You are a requirements analysis assistant. Please generate a "Balanced Summary" with approximately 20â€“30% of the content.

[Selection Rules]
- Introduce process skeletons and domain elements, but keep at a high-level overview.
- Each point should be 1â€“2 sentences; list lengths are limited to control the amount of information.

[Output format (Markdown)]
# Balanced Summary
- Goals and scope (2â€“3 sentences)
- Roles and user stories (â‰¤5 roles; total â‰¤6 user stories, format: "As a <role>, I want <action> so that <value>")
- Key processes (ordered list â‰¤7 steps; each step 1 sentence, indicate the trigger)
- Domain data elements (entities â‰¤6; for each entity, list the primary key and 3â€“5 key field names)
- Non-functional requirements (â‰¤6 items)
- Milestones and external dependencies (â‰¤5 items)
- Risks and mitigation strategies (â‰¤5 items)
- Undecided issues (â‰¤6 items; unknown write "Not mentioned")"""
    },
    "detailed": {
        "name": "Detailed Summary",
        "folder": "summary_detailed",
        "info_range": "30%-50%",
        "prompt_template": """You are a requirements analysis assistant. Please generate a "Detailed Summary" with approximately 30â€“50% of the content.

[Selection Rules]
- Expand to the level of detail suitable for review or task breakdown, but avoid implementing field-level details.
- Each point should be 1â€“3 sentences; processes should include main flow and key branches; interfaces should be summarized as bullet points.

[Output format (Markdown)]
# Detailed Summary
- Background and scope (3â€“5 sentences; including non-goals)
- Role matrix and use cases (â‰¤6 roles; main/exception scenarios â‰¤8 total)
- Business process (main process â‰¤8 steps; key branches â‰¤2, each â‰¤4 steps; indicate trigger/input/output)
- Domain model (entities â‰¤8; list field names with key constraints, such as "required/unique/reference")
- Interfaces and integrations (for each, write: system, direction, interaction points or theme, input key points, output key points, SLA key points; â‰¤8 total)
- Acceptance criteria (2â€“4 Given-When-Then per capability)
- Non-functional metrics (performance/reliability/security/compliance/observability; each â‰¤2 items)
- Milestones and release strategy (â‰¤6 items)
- Risk list and mitigation strategies (â‰¤8 items)
- Undecided issues and responsible parties (â‰¤8 items; unknown write "Not mentioned")"""
    }
}



def get_markdown_files(source_dir: Path) -> List[Path]:
    """è·å–æ‰€æœ‰Markdownæ–‡ä»¶"""
    return list(source_dir.glob("*.md"))


def read_file_content(file_path: Path) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        with print_lock:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return ""


def generate_summary(
    client: OpenAI,
    content: str,
    level_config: Dict,
    model: str = None,
    temperature: float = None
) -> str:
    """ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦"""
    system_prompt = level_config["prompt_template"]
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content}
            ],
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        with print_lock:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        return ""


def save_summary(output_path: Path, summary: str):
    """ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶"""
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        with print_lock:
            print(f"âœ… å·²ä¿å­˜: {output_path}")
    except Exception as e:
        with print_lock:
            print(f"âŒ ä¿å­˜å¤±è´¥ {output_path}: {e}")


def process_file(
    file_path: Path,
    source_dir: Path,
    output_dirs: Dict[str, Path],
    client_config: Dict,
    model: str,
    temperature: float,
    selected_levels: Optional[List[str]] = None,
    force: bool = False,
    file_index: int = 0,
    total_files: int = 0
):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œç”ŸæˆæŒ‡å®šçº§åˆ«çš„æ‘˜è¦"""
    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„OpenAIå®¢æˆ·ç«¯
    client = OpenAI(**client_config)
    
    with print_lock:
        if total_files > 0:
            print(f"\n[{file_index}/{total_files}] ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
        else:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    content = read_file_content(file_path)
    if not content:
        with print_lock:
            print(f"âš ï¸  è·³è¿‡ç©ºæ–‡ä»¶: {file_path.name}")
        return
    
    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
    relative_path = file_path.relative_to(source_dir)
    
    # ç¡®å®šè¦å¤„ç†çš„çº§åˆ«
    levels_to_process = selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())
    
    # ä¸ºæ¯ä¸ªæ‘˜è¦çº§åˆ«ç”Ÿæˆæ‘˜è¦
    for level_key in levels_to_process:
        if level_key not in SUMMARY_LEVELS:
            continue
        
        level_config = SUMMARY_LEVELS[level_key]
        output_dir = output_dirs[level_key]
        output_path = output_dir / relative_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if output_path.exists() and not force:
            with print_lock:
                print(f"  â­ï¸  {file_path.name}: {level_config['name']}å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        with print_lock:
            print(f"  ğŸ”„ {file_path.name}: ç”Ÿæˆ{level_config['name']} ({level_config['info_range']})...")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = generate_summary(
            client=client,
            content=content,
            level_config=level_config,
            model=model,
            temperature=temperature
        )
        
        if summary:
            # ä¿å­˜æ‘˜è¦
            save_summary(output_path, summary)
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™æµï¼ˆå¹¶å‘æ—¶æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å»¶è¿Ÿï¼‰
            time.sleep(0.5)
        else:
            with print_lock:
                print(f"  âš ï¸  {file_path.name}: {level_config['name']}ç”Ÿæˆå¤±è´¥")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="éœ€æ±‚æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ‘˜è¦çº§åˆ«é€‰é¡¹ï¼š
  ultra_short  - è¶…çŸ­æ‘˜è¦ï¼ˆ5%-10%ï¼‰
  short        - ç®€çŸ­æ‘˜è¦ï¼ˆ10%-20%ï¼‰
  balanced     - å¹³è¡¡æ‘˜è¦ï¼ˆ20%-30%ï¼‰
  detailed     - è¯¦å°½æ‘˜è¦ï¼ˆ30%-50%ï¼‰
  all          - æ‰€æœ‰çº§åˆ«ï¼ˆé»˜è®¤ï¼‰
        """
    )
    parser.add_argument(
        "--level",
        nargs="+",
        choices=["ultra_short", "short", "balanced", "detailed", "all"],
        default=["all"],
        help="é€‰æ‹©è¦ç”Ÿæˆçš„æ‘˜è¦çº§åˆ«ï¼ˆé»˜è®¤ï¼šallï¼‰"
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ‘˜è¦"
    )
    parser.add_argument(
        "--model",
        type=str,
        help="è¦†ç›–é»˜è®¤æ¨¡å‹é…ç½®"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="è¦†ç›–é»˜è®¤æ¸©åº¦å‚æ•°"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=1,
        help="å¹¶å‘å¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š1ï¼Œå³é¡ºåºå¤„ç†ï¼‰"
    )
    
    args = parser.parse_args()
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent
    source_dir = project_root / "req_md"
    
    # æ£€æŸ¥æºç›®å½•
    if not source_dir.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        sys.exit(1)
    
    # ç¡®å®šè¦å¤„ç†çš„çº§åˆ«
    if "all" in args.level:
        selected_levels = None  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰çº§åˆ«
        print("ğŸ“ å°†ç”Ÿæˆæ‰€æœ‰çº§åˆ«çš„æ‘˜è¦")
    else:
        selected_levels = args.level
        print(f"ğŸ“ å°†ç”Ÿæˆä»¥ä¸‹çº§åˆ«çš„æ‘˜è¦: {', '.join([SUMMARY_LEVELS[k]['name'] for k in selected_levels])}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåˆ›å»ºéœ€è¦çš„ï¼‰
    output_dirs = {}
    levels_to_create = selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())
    for level_key in levels_to_create:
        if level_key in SUMMARY_LEVELS:
            level_config = SUMMARY_LEVELS[level_key]
            output_dir = project_root / level_config["folder"]
            output_dir.mkdir(parents=True, exist_ok=True)
            output_dirs[level_key] = output_dir
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        sys.exit(1)
    
    model = args.model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    temperature = args.temperature if args.temperature is not None else float(os.getenv("OPENAI_TEMPERATURE", "0.3"))
    
    base_url = os.getenv("OPENAI_BASE_URL")
    client_config = {
        "api_key": api_key,
    }
    if base_url:
        client_config["base_url"] = base_url
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {temperature}")
    if args.force:
        print("ğŸ”„ å¼ºåˆ¶æ¨¡å¼ï¼šå°†é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ‘˜è¦")
    if args.workers > 1:
        print(f"âš¡ å¹¶å‘æ¨¡å¼ï¼šä½¿ç”¨ {args.workers} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    # è·å–æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = get_markdown_files(source_dir)
    
    # åº”ç”¨é™åˆ¶
    if args.limit:
        md_files = md_files[:args.limit]
        print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {args.limit} ä¸ªæ–‡ä»¶")
    
    total_files = len(md_files)
    
    if total_files == 0:
        print(f"âŒ åœ¨ {source_dir} ä¸­æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
        sys.exit(1)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {total_files} ä¸ªMarkdownæ–‡ä»¶\n")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    if args.workers > 1:
        # å¹¶å‘å¤„ç†æ¨¡å¼
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(
                    process_file,
                    file_path=file_path,
                    source_dir=source_dir,
                    output_dirs=output_dirs,
                    client_config=client_config,
                    model=model,
                    temperature=temperature,
                    selected_levels=selected_levels,
                    force=args.force,
                    file_index=idx,
                    total_files=total_files
                ): file_path
                for idx, file_path in enumerate(md_files, 1)
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                except Exception as e:
                    with print_lock:
                        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        # é¡ºåºå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        for idx, file_path in enumerate(md_files, 1):
            print(f"\n[{idx}/{total_files}]", end=" ")
            process_file(
                file_path=file_path,
                source_dir=source_dir,
                output_dirs=output_dirs,
                client_config=client_config,
                model=model,
                temperature=temperature,
                selected_levels=selected_levels,
                force=args.force,
                file_index=idx,
                total_files=total_files
            )
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    for level_key in (selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())):
        if level_key in output_dirs:
            level_config = SUMMARY_LEVELS[level_key]
            output_dir = output_dirs[level_key]
            print(f"  {level_config['name']}: {output_dir}")


if __name__ == "__main__":
    main()
