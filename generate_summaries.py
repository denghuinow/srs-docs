#!/usr/bin/env python3
"""
éœ€æ±‚æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå·¥å…·
æ”¯æŒ4ç§è¯¦ç»†ç¨‹åº¦ï¼šè¶…çŸ­æ‘˜è¦ã€ç®€çŸ­æ‘˜è¦ã€å¹³è¡¡æ‘˜è¦ã€è¯¦å°½æ‘˜è¦
"""

import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from typing import Dict, List, Optional
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ‰“å°é”ï¼Œç”¨äºçº¿ç¨‹å®‰å…¨çš„è¾“å‡º
print_lock = threading.Lock()

# æ‘˜è¦çº§åˆ«é…ç½®
SUMMARY_LEVELS = {
    "ultra_short": {
        "name": "Ultra Short Summary",
        "folder": "ultra_short",
        "info_range": "5%-10%",
        "prompt_file": "ultra_short.md"
    },
    "short": {
        "name": "Short Summary",
        "folder": "short",
        "info_range": "10%-20%",
        "prompt_file": "short.md"
    },
    "balanced": {
        "name": "Balanced Summary",
        "folder": "balanced",
        "info_range": "20%-30%",
        "prompt_file": "balanced.md"
    },
    "detailed": {
        "name": "Detailed Summary",
        "folder": "detailed",
        "info_range": "30%-50%",
        "prompt_file": "detailed.md"
    }
}



def _load_prompt_template(prompt_file: str, prompt_version: str = "v1") -> str:
    """
    åŠ è½½promptæ¨¡æ¿

    Args:
        prompt_file: æ¨¡æ¿æ–‡ä»¶å
        prompt_version: æç¤ºè¯ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º v1

    Returns:
        æ¨¡æ¿å†…å®¹
    """
    # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼šprompts/{version}/{prompt_file}
    project_root = Path(__file__).parent
    template_path = project_root / "prompts" / prompt_version / prompt_file
    
    if not template_path.exists():
        # å¦‚æœæŒ‡å®šç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨v1ä½œä¸ºé»˜è®¤ç‰ˆæœ¬
        if prompt_version != "v1":
            with print_lock:
                print(f"âš ï¸  æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {template_path}ï¼Œå°è¯•ä½¿ç”¨ v1 ç‰ˆæœ¬")
            template_path = project_root / "prompts" / "v1" / prompt_file
    
    if not template_path.exists():
        raise FileNotFoundError(
            f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}ï¼Œè¯·æ£€æŸ¥æç¤ºè¯ç‰ˆæœ¬å’Œæ–‡ä»¶è·¯å¾„"
        )
    return template_path.read_text(encoding="utf-8")


def get_markdown_files(source_dir: Path) -> List[Path]:
    """è·å–æ‰€æœ‰Markdownæ–‡ä»¶"""
    return list(source_dir.glob("*.md"))


def read_file_content(file_path: Path) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        with print_lock:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return ""


def generate_summary(
    client: OpenAI,
    content: str,
    level_config: Dict,
    model: str = None,
    temperature: float = None,
    prompt_version: str = "v1"
) -> str:
    """ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦"""
    # ä»æ–‡ä»¶åŠ è½½æç¤ºè¯æ¨¡æ¿
    prompt_file = level_config["prompt_file"]
    prompt_template = _load_prompt_template(prompt_file, prompt_version)
    
    # æ£€æŸ¥æ¨¡æ¿ä¸­æ˜¯å¦åŒ…å« {srs_text} å ä½ç¬¦
    if "{srs_text}" in prompt_template:
        # å¦‚æœåŒ…å«å ä½ç¬¦ï¼Œæ›¿æ¢å®ƒå¹¶å°†æ•´ä¸ªå†…å®¹ä½œä¸º user message
        user_message = prompt_template.replace("{srs_text}", content)
        messages = [
            {"role": "user", "content": user_message}
        ]
    else:
        # å¦‚æœæ²¡æœ‰å ä½ç¬¦ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ï¼šæ¨¡æ¿ä½œä¸º system promptï¼Œå†…å®¹ä½œä¸º user message
        messages = [
            {"role": "system", "content": prompt_template},
            {"role": "user", "content": content}
        ]
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        with print_lock:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        return ""


def save_summary(output_path: Path, summary: str):
    """ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶"""
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        with print_lock:
            print(f"âœ… å·²ä¿å­˜: {output_path}")
    except Exception as e:
        with print_lock:
            print(f"âŒ ä¿å­˜å¤±è´¥ {output_path}: {e}")


def process_file(
    file_path: Path,
    source_dir: Path,
    output_dirs: Dict[str, Path],
    client_config: Dict,
    model: str,
    temperature: float,
    selected_levels: Optional[List[str]] = None,
    force: bool = False,
    file_index: int = 0,
    total_files: int = 0,
    prompt_version: str = "v1"
):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œç”ŸæˆæŒ‡å®šçº§åˆ«çš„æ‘˜è¦"""
    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„OpenAIå®¢æˆ·ç«¯
    client = OpenAI(**client_config)
    
    with print_lock:
        if total_files > 0:
            print(f"\n[{file_index}/{total_files}] ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
        else:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    content = read_file_content(file_path)
    if not content:
        with print_lock:
            print(f"âš ï¸  è·³è¿‡ç©ºæ–‡ä»¶: {file_path.name}")
        return
    
    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
    relative_path = file_path.relative_to(source_dir)
    
    # ç¡®å®šè¦å¤„ç†çš„çº§åˆ«
    levels_to_process = selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())
    
    # ä¸ºæ¯ä¸ªæ‘˜è¦çº§åˆ«ç”Ÿæˆæ‘˜è¦
    for level_key in levels_to_process:
        if level_key not in SUMMARY_LEVELS:
            continue
        
        level_config = SUMMARY_LEVELS[level_key]
        output_dir = output_dirs[level_key]
        output_path = output_dir / relative_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if output_path.exists() and not force:
            with print_lock:
                print(f"  â­ï¸  {file_path.name}: {level_config['name']}å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        with print_lock:
            print(f"  ğŸ”„ {file_path.name}: ç”Ÿæˆ{level_config['name']} ({level_config['info_range']})...")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = generate_summary(
            client=client,
            content=content,
            level_config=level_config,
            model=model,
            temperature=temperature,
            prompt_version=prompt_version
        )
        
        if summary:
            # ä¿å­˜æ‘˜è¦
            save_summary(output_path, summary)
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™æµï¼ˆå¹¶å‘æ—¶æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å»¶è¿Ÿï¼‰
            time.sleep(0.5)
        else:
            with print_lock:
                print(f"  âš ï¸  {file_path.name}: {level_config['name']}ç”Ÿæˆå¤±è´¥")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="éœ€æ±‚æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ‘˜è¦çº§åˆ«é€‰é¡¹ï¼š
  ultra_short  - è¶…çŸ­æ‘˜è¦ï¼ˆ5%-10%ï¼‰
  short        - ç®€çŸ­æ‘˜è¦ï¼ˆ10%-20%ï¼‰
  balanced     - å¹³è¡¡æ‘˜è¦ï¼ˆ20%-30%ï¼‰
  detailed     - è¯¦å°½æ‘˜è¦ï¼ˆ30%-50%ï¼‰
  all          - æ‰€æœ‰çº§åˆ«ï¼ˆé»˜è®¤ï¼‰
        """
    )
    parser.add_argument(
        "--level",
        nargs="+",
        choices=["ultra_short", "short", "balanced", "detailed", "all"],
        default=["all"],
        help="é€‰æ‹©è¦ç”Ÿæˆçš„æ‘˜è¦çº§åˆ«ï¼ˆé»˜è®¤ï¼šallï¼‰"
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ‘˜è¦"
    )
    parser.add_argument(
        "--model",
        type=str,
        help="è¦†ç›–é»˜è®¤æ¨¡å‹é…ç½®"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="è¦†ç›–é»˜è®¤æ¸©åº¦å‚æ•°"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=1,
        help="å¹¶å‘å¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š1ï¼Œå³é¡ºåºå¤„ç†ï¼‰"
    )
    parser.add_argument(
        "--prompt-version",
        type=str,
        default="v1",
        help="æç¤ºè¯ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼šv1ï¼‰"
    )
    
    args = parser.parse_args()
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent
    source_dir = project_root / "resources" / "req_md"
    
    # æ£€æŸ¥æºç›®å½•
    if not source_dir.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        sys.exit(1)
    
    # ç¡®å®šè¦å¤„ç†çš„çº§åˆ«
    if "all" in args.level:
        selected_levels = None  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰çº§åˆ«
        print("ğŸ“ å°†ç”Ÿæˆæ‰€æœ‰çº§åˆ«çš„æ‘˜è¦")
    else:
        selected_levels = args.level
        print(f"ğŸ“ å°†ç”Ÿæˆä»¥ä¸‹çº§åˆ«çš„æ‘˜è¦: {', '.join([SUMMARY_LEVELS[k]['name'] for k in selected_levels])}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåˆ›å»ºéœ€è¦çš„ï¼‰
    output_dirs = {}
    levels_to_create = selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())
    for level_key in levels_to_create:
        if level_key in SUMMARY_LEVELS:
            level_config = SUMMARY_LEVELS[level_key]
            output_dir = project_root / "resources" / "summary" / level_config["folder"]
            output_dir.mkdir(parents=True, exist_ok=True)
            output_dirs[level_key] = output_dir
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        sys.exit(1)
    
    model = args.model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    temperature = args.temperature if args.temperature is not None else float(os.getenv("OPENAI_TEMPERATURE", "0.3"))
    
    base_url = os.getenv("OPENAI_BASE_URL")
    client_config = {
        "api_key": api_key,
    }
    if base_url:
        client_config["base_url"] = base_url
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {temperature}")
    print(f"ğŸ“ æç¤ºè¯ç‰ˆæœ¬: {args.prompt_version}")
    if args.force:
        print("ğŸ”„ å¼ºåˆ¶æ¨¡å¼ï¼šå°†é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ‘˜è¦")
    if args.workers > 1:
        print(f"âš¡ å¹¶å‘æ¨¡å¼ï¼šä½¿ç”¨ {args.workers} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    # è·å–æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = get_markdown_files(source_dir)
    
    # åº”ç”¨é™åˆ¶
    if args.limit:
        md_files = md_files[:args.limit]
        print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {args.limit} ä¸ªæ–‡ä»¶")
    
    total_files = len(md_files)
    
    if total_files == 0:
        print(f"âŒ åœ¨ {source_dir} ä¸­æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
        sys.exit(1)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {total_files} ä¸ªMarkdownæ–‡ä»¶\n")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    if args.workers > 1:
        # å¹¶å‘å¤„ç†æ¨¡å¼
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(
                    process_file,
                    file_path=file_path,
                    source_dir=source_dir,
                    output_dirs=output_dirs,
                    client_config=client_config,
                    model=model,
                    temperature=temperature,
                    selected_levels=selected_levels,
                    force=args.force,
                    file_index=idx,
                    total_files=total_files,
                    prompt_version=args.prompt_version
                ): file_path
                for idx, file_path in enumerate(md_files, 1)
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                except Exception as e:
                    with print_lock:
                        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        # é¡ºåºå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        for idx, file_path in enumerate(md_files, 1):
            print(f"\n[{idx}/{total_files}]", end=" ")
            process_file(
                file_path=file_path,
                source_dir=source_dir,
                output_dirs=output_dirs,
                client_config=client_config,
                model=model,
                temperature=temperature,
                selected_levels=selected_levels,
                force=args.force,
                file_index=idx,
                total_files=total_files,
                prompt_version=args.prompt_version
            )
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    for level_key in (selected_levels if selected_levels else list(SUMMARY_LEVELS.keys())):
        if level_key in output_dirs:
            level_config = SUMMARY_LEVELS[level_key]
            output_dir = output_dirs[level_key]
            print(f"  {level_config['name']}: {output_dir}")


if __name__ == "__main__":
    main()
